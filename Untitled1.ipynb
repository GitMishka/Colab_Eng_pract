{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUu9LvDBSYglhvyI7IVptl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GitMishka/Colab_Eng_pract/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPZD7JzzK3Dv"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ReadCSV\").getOrCreate()\n",
        "df = spark.read.csv(\"/path/to/your/file.csv\", header=True, inferSchema=True)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "selected_data = df.select(\"column1\", \"column2\")\n",
        "filtered_data = df.filter(col(\"column3\") > 100)\n",
        "transformed_data = df.withColumn(\"new_column\", col(\"column4\") * 2)"
      ],
      "metadata": {
        "id": "28eILYTFLvTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"my_table\")\n",
        "result = spark.sql(\"SELECT * FROM my_table WHERE column5 LIKE '%keyword%'\")\n",
        "result.show()"
      ],
      "metadata": {
        "id": "aHAn3pQPNGVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delta_df = spark.read.format(\"delta\").load(\"/path/to/delta/table\")\n",
        "transformed_delta_df = delta_df.filter(col(\"column6\") < 50)\n",
        "transformed_delta_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/delta/output\")"
      ],
      "metadata": {
        "id": "3p-gi20nNQw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_data.write.parquet(\"/path/to/output/directory\")"
      ],
      "metadata": {
        "id": "txMvjjbCM-hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
        "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\n",
        "classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=[indexer, assembler, classifier])\n",
        "\n",
        "model = pipeline.fit(training_data)\n",
        "\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "predictions.select(\"prediction\", \"probability\").show()\n"
      ],
      "metadata": {
        "id": "33Jjxcj-NhKo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}